{
  "name": "llm-proxy-server",
  "version": "1.0.0",
  "description": "Simple LLM proxy server with function calling support",
  "main": "server.js",
  "scripts": {
    "start": "node server.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "dotenv": "^16.3.1",
    "openai": "^4.28.0",
    "winston": "^3.11.0"
  }
}
